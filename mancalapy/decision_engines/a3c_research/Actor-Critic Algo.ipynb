{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.layers.fully_connected??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            1) fully connected NN to encode board state into \n",
    "            higher dimensional features. input_size = 17 (flattened board)\n",
    "            \n",
    "            2) LSTM takes previous output and outputs a new output\n",
    "            \n",
    "            3) num_outputs = action_space (7 possible actions)\n",
    "            \n",
    "            4) Initialize fully connected network for policy(softmax)\n",
    "            and value(regressor) functions\n",
    "            \n",
    "            5) Initialize weights for each layer\n",
    "            \n",
    "            6) train model\n",
    "        \"\"\"\n",
    "    def __call__(self, input_game_state):\n",
    "        \"\"\"\n",
    "            // Move\n",
    "            1) take the input_game_state and forward propagate it through pipeline\n",
    "            2) Softmax over result from pipeline\n",
    "            3) select valid action by sampling across the action distribution\n",
    "            generated by the network\n",
    "        \"\"\"\n",
    "    def fit(self):\n",
    "         \"\"\"\n",
    "        1) Initialize model\n",
    "        \n",
    "        2) select optimizer\n",
    "        \n",
    "        3) for each epoch\n",
    "             sync local params with global model\n",
    "             if just starting initialize params (c and h)\n",
    "             \n",
    "             collect values, log probabilities, rewards and entropies\n",
    "             for steps in each epoch:\n",
    "                pass game state to model and get logits and value\n",
    "                pass logits to softmax and log_softmax to get log_prob and prob\n",
    "                calculate entropy\n",
    "                add to entropy list\n",
    "                \n",
    "                calculate action from probs\n",
    "                calculate the lob prob given action\n",
    "                \n",
    "                execute action on game to get reward, next_state, \n",
    "                and boolean that describes if game over\n",
    "                \n",
    "                add new values to the values list\n",
    "                add log_probs to log_probs list\n",
    "                add rewards to rewards list\n",
    "                \n",
    "            initialize rewards as 0\n",
    "            \n",
    "            add rewards to values list\n",
    "            iterate over all generated rewards(reverse order)\n",
    "                # calculate advantage given discounted reward\n",
    "                # calculate policy_loss and value_loss given advantages\n",
    "                R = args.gamma * R + rewards[i]\n",
    "                advantage = R - values[i]\n",
    "                value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "                # Generalized Advantage Estimataion\n",
    "                delta_t = rewards[i] + args.gamma * \\\n",
    "                    values[i + 1].data - values[i].data\n",
    "                gae = gae * args.gamma * args.tau + delta_t\n",
    "\n",
    "                policy_loss = policy_loss - \\\n",
    "                    log_probs[i] * Variable(gae) - args.beta * entropies[i]\n",
    "            \n",
    "            update params given gradients of losses\n",
    "            push params up to the global model\n",
    "    \"\"\"\n",
    "        \n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            1) seed (Random nums)\n",
    "            2) game environment\n",
    "            3) Agent has ActorCriticNetwork( initial_state, )\n",
    "            \n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "class TrainParameters:\n",
    "    \"\"\"\n",
    "        1) loads any existing weights\n",
    "        2) Spawns separate worker copies of ActorCriticModel\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
