{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ActorCriticNetwork:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.scope_name = kwargs.get(\"scope_name\", \"global\")\n",
    "        self.scope = tf.variable_scope(self.scope_name, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "        # parse all the keyword args into the instance variables\n",
    "        self.num_outputs = kwargs.get(\"encoded_board_size\", 400)\n",
    "        self.num_of_actions = kwargs.get(\"num_of_actions\", 7)\n",
    "        self.lstm_layers = kwargs.get(\"lstm_layers\", 1)\n",
    "        self.lstm_size = kwargs.get(\"lstm_size\", 512)\n",
    "        self.inputs = kwargs.get(\"inputs\")\n",
    "        self.dropout_prob = kwargs.get(\"dropout_prob\")\n",
    "        self.critic_output = kwargs.get(\"critic_output\")\n",
    "        self.actor_output = kwargs.get(\"actor_output\")\n",
    "        self.init_graph = kwargs.get(\"init_graph\")\n",
    "        self.state_size = kwargs.get(\"state_size\", 17)\n",
    "        self.model_name = kwargs.get(\"param_file\")\n",
    "        self.state = None  # Will be set implicitly by tensorflow\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # initialize the computational graph\n",
    "        with self.scope:\n",
    "            self.initialize_scope(self.scope)\n",
    "            if self.model_name and os.path.isfile(f\"{self.model_name}.meta\"):\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(self.sess, self.model_name)\n",
    "                self.sess.run()\n",
    "            else:\n",
    "                self.init_graph = tf.global_variables_initializer()\n",
    "                self.sess.run(self.init_graph)\n",
    "\n",
    "    def initialize_scope(self, graph):\n",
    "        # Probability for dropout\n",
    "        self.inputs = tf.placeholder(tf.float32, [None, self.state_size],\n",
    "                                     name='inputs')  # Dimensions of this will be 1 X 17 for each of the states\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # Let the activation function be RELu, we can play with it later on\n",
    "        # Automatically creates weights with the help of the Xavier Initializer\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\n",
    "        encoded_inputs = tf.contrib.layers.fully_connected(self.inputs, self.num_outputs)\n",
    "        rnn_inputs = tf.expand_dims(encoded_inputs, [1])\n",
    "        # Forming an LSTM Layer\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=self.dropout_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * self.lstm_layers)\n",
    "\n",
    "        # Get the outputs and the final_state, which need to be fed in the Value and Policy network\n",
    "        # final state comprises of c and h\n",
    "        outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell, rnn_inputs, dtype=tf.float32, initial_state=self.state\n",
    "        )\n",
    "\n",
    "        # calculate the value of move\n",
    "        self.critic_output = tf.contrib.layers.fully_connected(\n",
    "            outputs, 1, activation_fn=None\n",
    "        )\n",
    "\n",
    "        # calculate the distribution of actions\n",
    "        self.actor_output = tf.contrib.layers.fully_connected(\n",
    "            outputs, self.num_of_actions, activation_fn=tf.nn.softmax\n",
    "        )\n",
    "\n",
    "    def __call__(self, game_board, *args, **kwargs) -> np.array:\n",
    "        feed = {\n",
    "            self.inputs: game_board,\n",
    "            self.dropout_prob: kwargs.get(\"dropout_prob\", 0.2),\n",
    "        }\n",
    "        critic_output, actor_output = self.sess.run([self.critic_output, self.actor_output], feed_dict=feed)\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        Closes session if open,\n",
    "        tensorflow already has a isOpen check\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Side(Enum):\n",
    "    NORTH = 0\n",
    "    SOUTH = 1\n",
    "\n",
    "    def opposite(self):\n",
    "        return Side.SOUTH if self == Side.NORTH else Side.NORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANKALAH = 7\n",
    "\n",
    "def play_hole(hole, board_copy, agent_side) -> bool:\n",
    "    seeds = board_copy[agent_side.value][hole]\n",
    "    board_copy[agent_side.value][hole] = 0\n",
    "    cur_hole = (hole + 1)\n",
    "    current_side = agent_side\n",
    "    while seeds > 1:\n",
    "        # only increment my mankalah\n",
    "        if current_side != agent_side and cur_hole == MANKALAH:\n",
    "            cur_hole = (cur_hole + 1) % 8\n",
    "            current_side = current_side.opposite()\n",
    "            continue\n",
    "        board_copy[current_side.value][cur_hole] += 1\n",
    "        if cur_hole > 6:\n",
    "            current_side = current_side.opposite()\n",
    "        cur_hole = (cur_hole + 1) % 8\n",
    "        seeds -= 1\n",
    "\n",
    "    opposite_hole = MANKALAH - 1 - hole\n",
    "    # check if we can capture opponents pieces\n",
    "    if cur_hole != MANKALAH and current_side == agent_side \\\n",
    "            and board_copy[current_side.value][cur_hole] == 0 \\\n",
    "            and board_copy[current_side.opposite().value][opposite_hole] > 0:\n",
    "        captured_seeds = board_copy[current_side.opposite().value][opposite_hole]\n",
    "        board_copy[current_side.opposite().value][opposite_hole] = 0\n",
    "        board_copy[current_side.value][MANKALAH] += captured_seeds + 1  # current seed\n",
    "        return False\n",
    "\n",
    "    board_copy[current_side.value][cur_hole] += 1\n",
    "    return current_side == agent_side and cur_hole == MANKALAH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    return np.array([[7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 0, 1]])\n",
    "\n",
    "\n",
    "def score(init_game_state, new_state):\n",
    "    \"\"\"\n",
    "    Returns the change in mankalah given a move\n",
    "    :param init_game_state:\n",
    "    :param new_state:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # init_side\n",
    "    if init_game_state[-1] == 1:\n",
    "        return init_game_state[15] - new_state[15]\n",
    "    return init_game_state[7] - new_state[7]\n",
    "\n",
    "\n",
    "def flatten_game(game_board, side):\n",
    "    \"\"\"\n",
    "    Flatten the game board and concatenate the side\n",
    "    and return a new numpy array with the side on the end\n",
    "\n",
    "    :param side:\n",
    "    :param game_board:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.expand_dims(np.concatenate((game_board.flatten(), [side.value])), axis=0)\n",
    "\n",
    "\n",
    "def game_over(board):\n",
    "    return np.sum(board[Side.NORTH.value][:-1]) == 0 or np.sum(board[Side.SOUTH.value][:-1]) == 0\n",
    "\n",
    "\n",
    "def step(init_game_state, action):\n",
    "    \"\"\"\n",
    "    Takes a game state transforms it into a format that playhole\n",
    "    can understand, executes playhole then returns the next game state,\n",
    "    reward and whether or not the game is over\n",
    "    :param init_game_state:\n",
    "    :param action:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    init_game_state = init_game_state[0]\n",
    "    # last index is side\n",
    "    side = Side(init_game_state[-1])\n",
    "    board = np.reshape(init_game_state[:-1], (2, 8))\n",
    "    repeat_go = play_hole(action, board, side)\n",
    "    side = side if repeat_go else side.opposite()\n",
    "    new_state = flatten_game(board, side)\n",
    "\n",
    "    # calculate reward\n",
    "    reward = score(init_game_state, new_state[0])\n",
    "\n",
    "    return new_state, reward, game_over(board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "class Worker(ActorCriticNetwork):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.model_path = kwargs.get(\"model_path\")\n",
    "        self.lr = kwargs.get(\"lr\", 0.001)\n",
    "        self.trainer = kwargs.get(\"trainer\", tf.train.AdamOptimizer(learning_rate=self.lr))\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sess = kwargs.get(\"sess\")\n",
    "\n",
    "    def initialize_scope(self, graph):\n",
    "        super().initialize_scope(graph)\n",
    "        # sync with global model\n",
    "        self.update_local_ops = update_target_graph('global', self.scope_name)\n",
    "\n",
    "        self.actions = tf.placeholder(dtype=tf.int32)\n",
    "        self.advantage = tf.placeholder(dtype=tf.float32)\n",
    "        self.delta_t = tf.placeholder(dtype=tf.float32)\n",
    "        self.gamma = tf.placeholder(dtype=tf.float32)\n",
    "        self.value_loss = tf.Variable(initial_value=0.0, dtype=tf.float32)\n",
    "        self.policy_loss = tf.Variable(initial_value=0.0,dtype=tf.float32)\n",
    "        self.generalized_advantage = tf.Variable(initial_value=1.0, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.generalized_advantage *= self.gamma\n",
    "        self.generalized_advantage += self.delta_t\n",
    "\n",
    "        # Loss functions\n",
    "        self.value_loss += 0.5 * tf.reduce_sum(tf.square(self.advantage))\n",
    "        \n",
    "        self.entropy = - tf.reduce_sum(self.actor_output * tf.log(self.actor_output))\n",
    "        self.policy_loss -= tf.reduce_sum(tf.log(self.actor_output) * self.generalized_advantage)\n",
    "        self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "        # Get gradients from local network using local losses\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "        self.gradients = tf.gradients(self.loss, local_vars)\n",
    "        self.var_norms = tf.global_norm(local_vars)\n",
    "        grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n",
    "\n",
    "        # Apply local gradients to global network\n",
    "        global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "        self.apply_grads = self.trainer.apply_gradients(zip(grads, global_vars))\n",
    "\n",
    "    def work(self, max_episode_length, gamma, saver):\n",
    "        total_steps = 0\n",
    "        sess = self.sess\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            episode_count = 0\n",
    "            for i in range(max_episode_length):\n",
    "                # copy the local params from the global network\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                game_over = False\n",
    "                init_game_state = reset()\n",
    "                episode_frames.append(init_game_state)\n",
    "                for stage in range(30):\n",
    "                    # get an action distribution and estimate value from policy\n",
    "                    action_distribution, estimated_value, rnn_state = sess.run(\n",
    "                        [self.actor_output, self.critic_output, self.state],\n",
    "                        feed_dict={\n",
    "                            self.inputs: init_game_state,\n",
    "                            self.dropout_prob: 0.2,\n",
    "                        })\n",
    "\n",
    "                    # select action\n",
    "                    action = int(np.argmax(action_distribution))\n",
    "\n",
    "                    # play action on game\n",
    "                    next_game_state, reward, game_over = step(init_game_state, action)\n",
    "\n",
    "                    # save game transition and estimated value\n",
    "                    episode_buffer.append(\n",
    "                        [\n",
    "                            init_game_state,\n",
    "                            action,\n",
    "                            reward,\n",
    "                            next_game_state,\n",
    "                            game_over,\n",
    "                            estimated_value\n",
    "                        ])\n",
    "\n",
    "                    # add reward for episode + update state\n",
    "                    episode_reward += reward\n",
    "                    init_game_state = next_game_state\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    value_loss, policy_loss, entropy_loss, gradients, variance = self.update_params(episode_buffer,\n",
    "                                                                                                    sess, gamma)\n",
    "                    logging.warning(\n",
    "                        'For episode 1: %s we have value loss: %s '\n",
    "                        'and policy loss: %s' % (episode_count, value_loss, policy_loss)\n",
    "                    )\n",
    "\n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count != 0:\n",
    "                    saver.save(sess, self.model_path + '/model-' + str(episode_count) + '.ckpt')\n",
    "                    print(\"Saved Model\")\n",
    "\n",
    "                episode_count += 1\n",
    "\n",
    "    def update_params(self, episode_buffer, sess, gamma):\n",
    "        total_reward = 0\n",
    "        total_generalized_advantage = 0\n",
    "        value_loss = 0\n",
    "        policy_loss = 0\n",
    "        entropy_loss = 0\n",
    "        gradients = 0\n",
    "        variance = 0\n",
    "        for i, episode in enumerate(reversed(episode_buffer[:-1])):\n",
    "            state, action, reward, value, next_value = episode[0], episode[1], episode[2], \\\n",
    "                                                        episode[5], episode_buffer[i + 1][5]\n",
    "                \n",
    "            total_reward =  total_reward * gamma + reward\n",
    "            advantage = total_reward - value\n",
    "            \n",
    "            delta_t = reward + gamma * next_value - value\n",
    "            \n",
    "            total_generalized_advantage = total_generalized_advantage * gamma + delta_t\n",
    "\n",
    "            sess.run([self.loss],\n",
    "                feed_dict={\n",
    "                    self.inputs: state,\n",
    "                    self.gamma : gamma,\n",
    "                    self.dropout_prob: 0.2,\n",
    "                    self.actions: action,\n",
    "                    self.advantage: advantage,\n",
    "                    self.delta_t: delta_t\n",
    "            })\n",
    "            \n",
    "        return value_loss, policy_loss, entropy_loss, gradients, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value worker_0_28/Variable\n\t [[Node: worker_0_28/Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@worker_0_28/Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](worker_0_28/Variable)]]\n\nCaused by op 'worker_0_28/Variable/read', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-79-196e5452eb36>\", line 9, in <module>\n    sess=master_network.sess, model_path=path_to_be_stored)\n  File \"<ipython-input-78-e9b0f22588e4>\", line 19, in __init__\n    super().__init__(*args, **kwargs)\n  File \"<ipython-input-6-da0d76c5ed84>\", line 29, in __init__\n    self.initialize_scope(self.scope)\n  File \"<ipython-input-78-e9b0f22588e4>\", line 31, in initialize_scope\n    self.value_loss = tf.Variable(initial_value=0.0, dtype=tf.float32)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value worker_0_28/Variable\n\t [[Node: worker_0_28/Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@worker_0_28/Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](worker_0_28/Variable)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value worker_0_28/Variable\n\t [[Node: worker_0_28/Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@worker_0_28/Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](worker_0_28/Variable)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-196e5452eb36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     worker = Worker(0, scope_name=f\"worker_0\", state_size=17, saver=saver,\n\u001b[1;32m      9\u001b[0m                     sess=master_network.sess, model_path=path_to_be_stored)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-e9b0f22588e4>\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self, max_episode_length, gamma, saver)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     value_loss, policy_loss, entropy_loss, gradients, variance = self.update_params(episode_buffer,\n\u001b[0;32m--> 106\u001b[0;31m                                                                                                     sess, gamma)\n\u001b[0m\u001b[1;32m    107\u001b[0m                     logging.warning(\n\u001b[1;32m    108\u001b[0m                         \u001b[0;34m'For episode 1: %s we have value loss: %s '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-e9b0f22588e4>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self, episode_buffer, sess, gamma)\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelta_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             })\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value worker_0_28/Variable\n\t [[Node: worker_0_28/Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@worker_0_28/Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](worker_0_28/Variable)]]\n\nCaused by op 'worker_0_28/Variable/read', defined at:\n  File \"/usr/local/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-79-196e5452eb36>\", line 9, in <module>\n    sess=master_network.sess, model_path=path_to_be_stored)\n  File \"<ipython-input-78-e9b0f22588e4>\", line 19, in __init__\n    super().__init__(*args, **kwargs)\n  File \"<ipython-input-6-da0d76c5ed84>\", line 29, in __init__\n    self.initialize_scope(self.scope)\n  File \"<ipython-input-78-e9b0f22588e4>\", line 31, in initialize_scope\n    self.value_loss = tf.Variable(initial_value=0.0, dtype=tf.float32)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value worker_0_28/Variable\n\t [[Node: worker_0_28/Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@worker_0_28/Variable\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](worker_0_28/Variable)]]\n"
     ]
    }
   ],
   "source": [
    "path_to_be_stored = \".\"\n",
    "max_episodes = 2\n",
    "gamma = 0.3\n",
    "workers = []\n",
    "master_network = ActorCriticNetwork()\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    worker = Worker(0, scope_name=f\"worker_0\", state_size=17, saver=saver,\n",
    "                    sess=master_network.sess, model_path=path_to_be_stored)\n",
    "    worker.work(max_episodes, gamma=gamma, saver=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
