{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ActorCriticNetwork:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.scope_name = kwargs.get(\"scope_name\", \"global\")\n",
    "        self.scope = tf.variable_scope(self.scope_name, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "        # parse all the keyword args into the instance variables\n",
    "        self.num_outputs = kwargs.get(\"encoded_board_size\", 400)\n",
    "        self.num_of_actions = kwargs.get(\"num_of_actions\", 7)\n",
    "        self.lstm_layers = kwargs.get(\"lstm_layers\", 1)\n",
    "        self.lstm_size = kwargs.get(\"lstm_size\", 512)\n",
    "        self.inputs = kwargs.get(\"inputs\")\n",
    "        self.dropout_prob = kwargs.get(\"dropout_prob\")\n",
    "        self.critic_output = kwargs.get(\"critic_output\")\n",
    "        self.actor_output = kwargs.get(\"actor_output\")\n",
    "        self.init_graph = kwargs.get(\"init_graph\")\n",
    "        self.state_size = kwargs.get(\"state_size\", 17)\n",
    "        self.model_name = kwargs.get(\"param_file\")\n",
    "        self.state = None  # Will be set implicitly by tensorflow\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        # initialize the computational graph\n",
    "        with self.scope:\n",
    "            self.initialize_scope(self.scope)\n",
    "            if self.model_name and os.path.isfile(f\"{self.model_name}.meta\"):\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(self.sess, self.model_name)\n",
    "                self.sess.run()\n",
    "            else:\n",
    "                self.init_graph = tf.global_variables_initializer()\n",
    "                self.sess.run(self.init_graph)\n",
    "\n",
    "    def initialize_scope(self, graph):\n",
    "        # Probability for dropout\n",
    "        self.inputs = tf.placeholder(tf.float32, [self.state_size, 1],\n",
    "                                     name='inputs')  # Dimensions of this will be 17 X 1 for each of the states\n",
    "        self.dropout_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        # Let the activation function be RELu, we can play with it later on\n",
    "        # Automatically creates weights with the help of the Xavier Initializer\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\n",
    "        encoded_inputs = tf.contrib.layers.fully_connected(self.inputs, self.num_outputs)\n",
    "        rnn_inputs = tf.reshape(encoded_inputs, (1, self.state_size, self.num_outputs))\n",
    "        # Forming an LSTM Layer\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=self.dropout_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * self.lstm_layers)\n",
    "\n",
    "        # Get the outputs and the final_state, which need to be fed in the Value and Policy network\n",
    "        # final state comprises of c and h\n",
    "        outputs, self.state = tf.nn.dynamic_rnn(\n",
    "            cell, rnn_inputs, dtype=tf.float32, initial_state=self.state\n",
    "        )\n",
    "\n",
    "        # calculate the value of move\n",
    "        final_output = tf.expand_dims(outputs[0, 16, :], [0])\n",
    "        self.critic_output = tf.contrib.layers.fully_connected(\n",
    "            final_output, 1, activation_fn=None\n",
    "        )\n",
    "\n",
    "        # calculate the distribution of actions\n",
    "        self.actor_output = tf.contrib.layers.fully_connected(\n",
    "            final_output, self.num_of_actions, activation_fn=tf.nn.softmax\n",
    "        )\n",
    "\n",
    "    def __call__(self, game_board, *args, **kwargs) -> np.array:\n",
    "        feed = {\n",
    "            self.inputs: game_board.T,\n",
    "            self.dropout_prob: kwargs.get(\"dropout_prob\", 0.2),\n",
    "        }\n",
    "        critic_output, actor_output = self.sess.run([self.critic_output, self.actor_output], feed_dict=feed)\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def exit(self):\n",
    "        \"\"\"\n",
    "        Closes session if open,\n",
    "        tensorflow already has a isOpen check\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Side(Enum):\n",
    "    NORTH = 0\n",
    "    SOUTH = 1\n",
    "\n",
    "    def opposite(self):\n",
    "        return Side.SOUTH if self == Side.NORTH else Side.NORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANKALAH = 7\n",
    "\n",
    "def play_hole(hole, board_copy, agent_side) -> bool:\n",
    "    seeds = board_copy[agent_side.value][hole]\n",
    "    board_copy[agent_side.value][hole] = 0\n",
    "    cur_hole = (hole + 1)\n",
    "    current_side = agent_side\n",
    "    while seeds > 1:\n",
    "        # only increment my mankalah\n",
    "        if current_side != agent_side and cur_hole == MANKALAH:\n",
    "            cur_hole = (cur_hole + 1) % 8\n",
    "            current_side = current_side.opposite()\n",
    "            continue\n",
    "        board_copy[current_side.value][cur_hole] += 1\n",
    "        if cur_hole > 6:\n",
    "            current_side = current_side.opposite()\n",
    "        cur_hole = (cur_hole + 1) % 8\n",
    "        seeds -= 1\n",
    "\n",
    "    opposite_hole = MANKALAH - 1 - hole\n",
    "    # check if we can capture opponents pieces\n",
    "    if cur_hole != MANKALAH and current_side == agent_side \\\n",
    "            and board_copy[current_side.value][cur_hole] == 0 \\\n",
    "            and board_copy[current_side.opposite().value][opposite_hole] > 0:\n",
    "        captured_seeds = board_copy[current_side.opposite().value][opposite_hole]\n",
    "        board_copy[current_side.opposite().value][opposite_hole] = 0\n",
    "        board_copy[current_side.value][MANKALAH] += captured_seeds + 1  # current seed\n",
    "        return False\n",
    "\n",
    "    board_copy[current_side.value][cur_hole] += 1\n",
    "    return current_side == agent_side and cur_hole == MANKALAH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    return np.array([[7, 7, 7, 7, 7, 7, 7, 0, 7, 7, 7, 7, 7, 7, 7, 0, 1]])\n",
    "\n",
    "\n",
    "def score(init_game_state, new_state):\n",
    "    \"\"\"\n",
    "    Returns the change in mankalah given a move\n",
    "    :param init_game_state:\n",
    "    :param new_state:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # init_side\n",
    "    if init_game_state[-1] == 1:\n",
    "        return init_game_state[15] - new_state[15]\n",
    "    return init_game_state[7] - new_state[7]\n",
    "\n",
    "\n",
    "def flatten_game(game_board, side):\n",
    "    \"\"\"\n",
    "    Flatten the game board and concatenate the side\n",
    "    and return a new numpy array with the side on the end\n",
    "\n",
    "    :param side:\n",
    "    :param game_board:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.expand_dims(np.concatenate((game_board.flatten(), [side.value])), axis=0)\n",
    "\n",
    "\n",
    "def game_over(board):\n",
    "    return np.sum(board[Side.NORTH.value][:-1]) == 0 or np.sum(board[Side.SOUTH.value][:-1]) == 0\n",
    "\n",
    "\n",
    "def step(init_game_state, action):\n",
    "    \"\"\"\n",
    "    Takes a game state transforms it into a format that playhole\n",
    "    can understand, executes playhole then returns the next game state,\n",
    "    reward and whether or not the game is over\n",
    "    :param init_game_state:\n",
    "    :param action:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    init_game_state = init_game_state[0]\n",
    "    # last index is side\n",
    "    side = Side(init_game_state[-1])\n",
    "    board = np.reshape(init_game_state[:-1], (2, 8))\n",
    "    repeat_go = play_hole(action, board, side)\n",
    "    side = side if repeat_go else side.opposite()\n",
    "    new_state = flatten_game(board, side)\n",
    "\n",
    "    # calculate reward\n",
    "    reward = score(init_game_state, new_state[0])\n",
    "\n",
    "    return new_state, reward, game_over(board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "class Worker(ActorCriticNetwork):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.model_path = kwargs.get(\"model_path\")\n",
    "        self.lr = kwargs.get(\"lr\", 0.001)\n",
    "        self.trainer = kwargs.get(\"trainer\", tf.train.AdamOptimizer(learning_rate=self.lr))\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sess = kwargs.get(\"sess\")\n",
    "\n",
    "    def initialize_scope(self, graph):\n",
    "        super().initialize_scope(graph)\n",
    "        # sync with global model\n",
    "        self.update_local_ops = update_target_graph('global', self.scope_name)\n",
    "\n",
    "        self.actions = tf.placeholder(dtype=tf.int32)\n",
    "        self.target_v = tf.placeholder(dtype=tf.float32)\n",
    "        # self.advantages = tf.placeholder(dtype=tf.float32)\n",
    "        self.generalized_advantage = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "        # Loss functions\n",
    "        self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.critic_output, [-1])))\n",
    "        self.entropy = - tf.reduce_sum(self.actor_output * tf.log(self.actor_output))\n",
    "        self.policy_loss = -tf.reduce_sum(tf.log(self.actor_output) * self.generalized_advantage)\n",
    "        self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "        # Get gradients from local network using local losses\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope_name)\n",
    "        self.gradients = tf.gradients(self.loss, local_vars)\n",
    "        self.var_norms = tf.global_norm(local_vars)\n",
    "        grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n",
    "\n",
    "        # Apply local gradients to global network\n",
    "        global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "        self.apply_grads = self.trainer.apply_gradients(zip(grads, global_vars))\n",
    "\n",
    "    def work(self, max_episode_length, gamma, saver):\n",
    "        total_steps = 0\n",
    "        sess = self.sess\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            episode_count = 0\n",
    "            for i in range(max_episode_length):\n",
    "                # copy the local params from the global network\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                game_over = False\n",
    "\n",
    "                init_game_state = reset()\n",
    "                episode_frames.append(init_game_state)\n",
    "                while not game_over:\n",
    "                    # get an action distribution and estimate value from policy\n",
    "                    action_distribution, estimated_value, rnn_state = sess.run(\n",
    "                        [self.actor_output, self.critic_output, self.state],\n",
    "                        feed_dict={\n",
    "                            self.inputs: init_game_state.T,\n",
    "                            self.dropout_prob: 0.2,\n",
    "                            # self.state: rnn_state,\n",
    "                        })\n",
    "\n",
    "                    # select action\n",
    "                    action = int(np.argmax(action_distribution))\n",
    "\n",
    "                    # play action on game\n",
    "                    next_game_state, reward, game_over = step(init_game_state, action)\n",
    "\n",
    "                    # save game transition and estimated value\n",
    "                    episode_buffer.append(\n",
    "                        [\n",
    "                            init_game_state,\n",
    "                            action,\n",
    "                            reward,\n",
    "                            next_game_state,\n",
    "                            game_over,\n",
    "                            estimated_value\n",
    "                        ])\n",
    "\n",
    "                    # add reward for episode + update state\n",
    "                    episode_reward += reward\n",
    "                    init_game_state = next_game_state\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and not game_over and episode_step_count != max_episode_length - 1:\n",
    "                        self.update_params(\n",
    "                            episode_buffer, sess, gamma\n",
    "                        )\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "\n",
    "                # Update the network using the episode buffer at the end of the episode.\n",
    "                if len(episode_buffer) != 0:\n",
    "                    value_loss, policy_loss, entropy_loss, gradients, variance = self.update_params(episode_buffer,\n",
    "                                                                                                    sess, gamma)\n",
    "                    logging.warning(\n",
    "                        'For episode 1: %s we have value loss: %s '\n",
    "                        'and policy loss: %s' % (episode_count, value_loss, policy_loss)\n",
    "                    )\n",
    "\n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count != 0:\n",
    "                    saver.save(sess, self.model_path + '/model-' + str(episode_count) + '.ckpt')\n",
    "                    print(\"Saved Model\")\n",
    "\n",
    "                episode_count += 1\n",
    "\n",
    "    def update_params(self, episode_buffer, sess, gamma):\n",
    "        total_reward = 0\n",
    "        total_generalized_advantage = 0\n",
    "        value_loss = 0\n",
    "        policy_loss = 0\n",
    "        entropy_loss = 0\n",
    "        gradients = 0\n",
    "        variance = 0\n",
    "        for i, episode in enumerate(episode_buffer[:-1]):\n",
    "            state, action, reward, value = episode[0], episode[1], episode[2], episode[5]\n",
    "            total_reward += reward * (gamma ** i)\n",
    "            delta_t = reward + gamma * episode_buffer[i + 1][5] - value\n",
    "            total_generalized_advantage = total_generalized_advantage * gamma + delta_t\n",
    "\n",
    "            value_loss, policy_loss, entropy_loss, gradients, variance = sess.run([\n",
    "                self.value_loss, self.policy_loss, self.loss, self.gradients, self.var_norms], feed_dict={\n",
    "                self.inputs: state.T,\n",
    "                self.dropout_prob: 0.2,\n",
    "                self.actions: action,\n",
    "                self.target_v: total_reward,\n",
    "                self.generalized_advantage: total_generalized_advantage\n",
    "            })\n",
    "        return value_loss, policy_loss, entropy_loss, gradients, variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 0 we have value loss: 0.0105773 and policy loss: -2.21698\n",
      "WARNING:root:For episode 1: 1 we have value loss: 0.0930883 and policy loss: -7.99015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 2 we have value loss: 0.560992 and policy loss: -12.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 3 we have value loss: 0.0459874 and policy loss: -2.47914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 4 we have value loss: 5.63506e-05 and policy loss: 1.60753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 5 we have value loss: 3.6915e-05 and policy loss: 1.68309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 6 we have value loss: 0.56083 and policy loss: -16.4323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 7 we have value loss: 0.0129177 and policy loss: -13.9618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 8 we have value loss: 0.0895227 and policy loss: -19.4397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 9 we have value loss: 0.00470757 and policy loss: 0.272891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 10 we have value loss: 0.00416837 and policy loss: 1.78075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 11 we have value loss: 0.354776 and policy loss: 3.76032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:For episode 1: 12 we have value loss: 0.010455 and policy loss: -12.3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-61bca3a30093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     worker = Worker(0, scope_name=f\"worker_0\", state_size=17, saver=saver,\n\u001b[1;32m      9\u001b[0m                     sess=master_network.sess, model_path=path_to_be_stored)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6c6826d2ffef>\u001b[0m in \u001b[0;36mwork\u001b[0;34m(self, max_episode_length, gamma, saver)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame_over\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepisode_step_count\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmax_episode_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         self.update_params(\n\u001b[0;32m--> 101\u001b[0;31m                             \u001b[0mepisode_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                         )\n\u001b[1;32m    103\u001b[0m                         \u001b[0mepisode_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6c6826d2ffef>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self, episode_buffer, sess, gamma)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_v\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneralized_advantage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_generalized_advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             })\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_to_be_stored = \".\"\n",
    "max_episodes = 200\n",
    "gamma = 0.3\n",
    "workers = []\n",
    "master_network = ActorCriticNetwork()\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    worker = Worker(0, scope_name=f\"worker_0\", state_size=17, saver=saver,\n",
    "                    sess=master_network.sess, model_path=path_to_be_stored)\n",
    "    worker.work(max_episodes, gamma=gamma, saver=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}